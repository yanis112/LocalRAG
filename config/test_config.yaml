#config path
config_path: "config/config.yaml"

# Database initialization parameters
vectordb_provider: "Qdrant" # Provider of the vector database (Qdrant or Faiss)
persist_directory: "data/vector_stores/qdrant_semantic_test_2" #"data/vector_stores/qdrant_semantic_bge_jinai"  #Directory to store the index (give the name you want, just dont change "data/vector_stores/"  # Method to split the documents into chunks (semantic or "recursive")
path: "data/politique" # Path of the root folder of the documents you want to put in the index 
process_log_file: "processed_docs.log"
collection_name: "qdrant_vectorstore" # Name of the vector collection

#Knowledge graph index parameters
build_knowledge_graph: false  # Enable or disable the building of the knowledge graph when initializing the database
nb_nodes: 1  # Number of nodes to retrieve from the knowledge graph when asking a query (analogous to the number of chunks)
allow_kg_retrieval: false  # Enable or disable the retrieval of info from the knowledge graph during the generation
kg_target_context: 1500
min_relations_pct: 0.1
min_descriptions_pct: 0.2
min_communities_pct: 0.2


# Chunking parameters (only for recursive splitting)
splitting_method: "constant"  # Method to split the documents into chunks (semantic or "recursive")
chunk_size: 5000  # Chunk size for the documents in ters of number of characters (only for recursive splitting)
chunk_overlap: 0  # Overlap between chunks in terms of number of characters (only for recursive splitting)
semantic_threshold: 65  # Threshold for the semantic similarity between two chunks #75 avant
chunking_embedding_model: "all-MiniLM-L6-v2" #"BAAI/bge-m3" #"all-MiniLM-L6-v2" #"BAAI/bge-m3" #"ollama/bge-m3"     #BAAI/bge-m3"  # Embedding model to use for the chunking

# Database cloning parameters
clone_database: false  # Enable or disable the cloning of chunks of an existing index when initializing a database
clone_persist: "./data/vector_stores/chroma_semantic_mapnet_v2"  # Directory from which to clone the index
clone_embedding_model: "all-mpnet-base-v2"  # Embedding of the index from which to clone

# Search parameters
nb_chunks: 7  # Number of chunks to retrieve for the database (not taken into account is using reranker target)
search_type: "similarity"  # Type of search to use for the documents (similarity or exact)
hybrid_search: false  # Enable or disable the hybrid search (keyword + similarity)
length_threshold: 40  # Threshold for the length of the chunks (number of words)
use_multi_query: false  # Enable or disable the multi-query
advanced_hybrid_search: true # Enable or disable the advanced hybrid search (semantic + lexical, merging the results)
deep_search: false # Enable or disable the advanced RAG answer (Multi step answer generation)
alpha: 0.5 # Alpha parameter for the Advanced Hybrid Search
enable_routing: false # Enable or disable the routing of the queries to lexical/semantic search
suggestions_enabled: false # Enable or disable the query suggestions
use_history: false # Enable or disable the history of the chat to be used for answering the queries
chat_history: 'no_chat' # Chat history to use for the queries
data_sources: {
    "politique": "Political Content, expert opinions on political, geopolitical and economic issues",
    "emails": "Emails, chat messages, and other text-based communication",
    "jobs": "job descriptions, or job offers",
    "food": "Food,food recipes, and cooking instructions",
    "linkedin": "LinkedIn posts, technical articles, data science and AI content",
    "prompts": "refined prompts made by the user",
    "sheets": "Google sheets, excel files, and other tabular data",
    "charts": "Charts, graphs, and diagrams containing curves, lines, tendencies showing economic, political, or social data",
}

   
# Search filters
use_autocut: true  # Enable or disable the autocut
autocut_beta: 2.5  # Beta parameter for the autocut (number of std that the downward movement is allowed)
filter_on_length: true  # Filter the chunks based on the length (number of words)
enable_source_filter: true  # Enable or disable the source filter
word_filter: alice  # Filter the results based on the keyword
source_filter: []  # Filter the results based on the source
source_filter_type: "$eq"  # Type of source filter (include or exclude)
field_filter: []  # Field to filter on (source or length)
field_filter_type: "$eq"  # Type of field filter (include or exclude)


# Embedding models parameters
embedding_model: "Snowflake/snowflake-arctic-embed-l-v2.0" #"jinaai/jina-embeddings-v3" #"Snowflake/snowflake-arctic-embed-l-v2.0" #"jinaai/jina-embeddings-v3" #"BAAI/bge-m3" #"jinaai/jina-embeddings-v3" # "BAAI/bge-m3" #"jinaai/jina-embeddings-v3" #"all-MiniLM-L6-v2" #"BAAI/bge-m3" #"BAAI/bge-m3" #"all-mpnet-base-v2" #"all-MiniLM-L6-v2" #"BAAI/bge-m3" #"sentence_transformers/all-MiniLM-L6-v2" #"BAAI/bge-m3" #"ollama/bge-m3"  #"BAAI/bge-m3" #Embedding model to use for vectorstore creation, retrieval and semantic chunking
dense_embedding_size: 1024 #Size of the dense embedding model
sparse_embedding_model: "Qdrant/bm42-all-minilm-l6-v2-attentions" #Embedding model to use for sparse retrieval
sparse_embedding_size: 30522 #Size of the sparse embedding model


# Reranker parameters
reranker_token_target: 8000 #We give documents to the reranker until this number of tokens is reached (sum of the tokens of the documents)
token_compression: false  # Enable or disable the token compressor
nb_rerank: 5  # Number of documents to rerank (input 10 documents, output 5 documents)
use_reranker: true # Enable or disable the reranker
reranker_model: "jinaai/jina-reranker-v2-base-multilingual" #"BAAI/bge-reranker-large" #"jinaai/jina-reranker-v2-base-multilingual" # Reranker model to us

# Auto-merging parameters
auto_merging: false  # Enable or disable the automatic merging of the chunks
auto_merging_threshold: 7  # Threshold for the merging of the chunks (number of concomitant sources)


# Generation parameters
llm_provider: "groq" #"github" #"github" #"groq" #"sambanova" #"sambanova" #"groq" # Provider of the LLM model ('cerebras or ollama or groq') don't forget to change the provider if you change the llm. Pay attention the model name can vary depending on the provider !
model_name: "gemini-2.0-flash-exp" #"meta-Llama-3.1-405B-Instruct" #"llama3-405b" #gpt-4o" #llama-3.1-70b-versatile" #"llama-3.2-90b-text-preview" #"llama3-405b" # "llama-3.2-90b-text-preview" #"llama3-405b" #LLM used for inference the name can be in the following list, pay attention that the llm name should correspond to the llm provider: "hermes3" (ollama);"llama-3.1-8b-instant (groq)"
models_dict: {"gemini-2.0-flash-exp":"google",
              "deepseek-r1-distill-llama-70b":"groq",
              "llama-3.3-70b-versatile":"groq",
              "gpt-4o": "github",
              "meta-Llama-3.1-405B-Instruct": "github",
              "gpt-4o-mini": "github",
              "o1-mini": "github",
              "o1-preview": "github",
              "llama3-405b": "sambanova",
              "llama-3.1-8b-instant": "groq",
              "llama-3.2-3b-preview": "groq",
              "gemini-1.5-pro":"google",
              "gemini-1.5-flash":"google",
              "gemini-exp-1206":"google",
              "DeepSeek-R1":"github",
              "gemini-2.0-flash-thinking-exp-01-21":"google"
              
}

#Prompting parameters
fr_rag_prompt_path: "prompts/base_rag_prompt_fr.txt" #Path to the file containing the RAG prompt
en_rag_prompt_path: "prompts/base_rag_prompt_en.txt" #Path to the file containing the RAG prompt

#vllm parameters
vllm_model_name: "gemini-exp-1206" #VLLM model to use for the generation
vllm_provider: "gemini" # Provider of the VLLM model

#LLM parameters
cot_enabled: false  # Enable or disable the Chain-Of-Thoughts (COT) feature for the LLM answer
stream: false  # Enable or disable the streaming of the generation (if all the output text is generated and then printed in the interface or if the tokens are generated and printed one by one)
temperature: 1  # Temperature for the generation
llm_token_target: 0 # We give documents to the llm until this number of tokens is reached (sum of the tokens of the documents) , 0 means we don't use this parameter, if autocut is enabled, this parameter is not used !!
save_answer: false # Enable or disable the saving of the answer
prompt_language: "en"  # Language used for all the prompts (and consequently the LLM)

# EVAL parameters
top_k: 5  # Evaluation is valid if the ground truth is in the top k chunks retrieved (you cannot modify this parameter for now)
evaluate_generation: false  # Enable or disable the evaluation of the generation (only evaluate the retrieval if disabled)
answer_relevancy_llm: "gemma2" # LLM model to use for generating the artificial queries
answer_relevancy_provider: "ollama"  # Provider of the LLM model for the artificial queries
answer_relevancy_embedding: "BAAI/bge-m3"  # Embedding model to use for the artificial queries
metrics_list: ["latency","answer_relevancy","hallucination_score"] # List of metrics to evaluate, if a metric is not in the list, it is not computed

#Entity/Relation Extraction parameters
entity_model_name: "knowledgator/gliner-poly-small-v1.0" #Entity extraction model, can be in: "knowledgator/gliner-poly-base-v1.0" #"knowledgator/gliner-bi-large-v1.0" #"gliner-community/gliner_large-v2.5" (check on Huggingface for the latest models)
relation_model_name: "knowledgator/gliner-multitask-large-v0.5" #Relation extraction model, can be in: "knowledgator/gliner-multitask-large-v0.5" (check on Huggingface for the latest models)
allowed_entities_path: "src/allowed_entities.json" #Path to the file containing the allowed entity types (you can modify this file)
allowed_relations_path: "src/allowed_relations.json" #Path to the file containing the allowed relations types (you can modify this file)
allowed_precise_relations_path: "src/allowed_detailled_relations.json" #Path to the file containing the allowed precise relations types (you can modify this file)
entity_detection_threshold: 0.5 #Threshold for the entity detection (be careful when changing this parameter)
relation_extraction_threshold: 0.5  #Threshold for the relation extraction (be careful when changing this parameter)
disambiguate_threshold: 50 #Threshold for the disambiguation of the entities (be careful when changing this parameter)

#Community summarisation / entity description parameters
description_model_name: "gemma2:2b" #LLM model to use for generating the entity descriptions (smaller LLMs are faster but less accurate)
description_llm_provider: "ollama" # Provider of the LLM model for the entity descriptions

# Possible actions
actions_dict: {
    "employer contact writing": "Writing professional communication like cover letters and outreach messages to potential employers",
    "email support inquiry": "User asks a question (whatever the subject), asking explicitely to use emails as information source.",
    "job search assistance": "Providing guidance and help with finding job opportunities, including search strategies and job board navigation",
    "document content question": "Addressing general queries about document content, or any particular request that is not covered properly by the other actions",
    "graph creation request": "User asks explicitelly to create charts, graphs, and diagrams",
    "meeting notes summarization": "Converting meeting recordings or raw notes into concise, structured summaries",
    #"previous answer correction": "User asks to correct a previous answer, modifying an already created content, shortening or expanding it, or changing the content or wanting to add more information that was not included in the previous answer (e.g. user starts his prompt with 'no, more details about...','make this shorter', 'no i search for...') the object of the modification is not explicitely present in his query.",
    "prompt engineering request": "User asks the system to generate a detailed prompt for articial image creation, refining the prompt until it meets the user's needs",
    "political question": "User asks a question about political, geopolitical, or economic issues, or about a personnality, or ask the assistant to do a task related to political thinking",
    "sheet or table info extraction":"User asks to extract information from a table or a sheet, like google sheets, excel files, or any tabular data",
}

#Agentic RAG parameters
query_breaker_model: "o1-mini"
query_breaker_provider: "github"

#Intent classifier parameters
query_classification_model: "gemini-2.0-flash-exp"
query_classification_provider: "google"

#Transcription parameters
transcription_chunk_size: 1800 #corresponds to 30 mins of audio per chunks (size is limited to 25Mb /request)